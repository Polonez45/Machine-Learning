{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word2Vec Tutorial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Based on Lecture 2 from the Stanford Deep Learning Series by Christopher Manning__\n",
    "* __Word2Vec: Set of algorithms developed by Mikolov et al. (2013)__\n",
    "* __Original Word2Vec Paper: https://arxiv.org/abs/1301.3781__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How do we represent the meaning of a word computationally?\n",
    "* Often we make use of a taxonomy like WordNet, using hypernym relationships and synonym sets.\n",
    "* Problem with this:\n",
    "    * Misses nuances (eg. 'good' and 'expert' are considered synonyms in WordNet, but \"he's an expert at deep learning\" and \"he's good at deep learning\" clearly have different meanings).\n",
    "    * Misses new words (must constantly keep up to date), requires a lot of human labor.\n",
    "    \n",
    "    \n",
    "* This can be considered a 'discrete' representation, using atomic symbols.\n",
    "* Words like 'hotel', 'conference', 'walk', in vector space terms are represented using a vector of one 1 and many zeros [0 0 0 0 0 0 0 0 1 0 0 0], thus a 'one-hot' vector representation.\n",
    "* These vectors end up being very long, depending on the vocabulary.\n",
    "* Another problem: doesn't show any inherent relationship between words, eg. 'Seattle motel' and 'Seattle hotel' should be considered similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better approach:\n",
    "* Find a way for the vectors to directly encode similarity &mdash; distributional similarity (the meaning of a word is the context it is in).\n",
    "* \"You shall know a word by the company it keeps.\" J. R. Firth 1957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Our goal:__ build a dense vector for each word, chosen so that it is good at predicting other words appearing in its context &mdash; the other words also represented by vectors of their own.\n",
    "* Eg. linguistics = [0.286 0.792 -0.177 -0.107 0.109 -0.542 0.349 0.271]\n",
    "* This type of dense vector representation is distributed &mdash; instead of an atomic 'one-hot' vector representation, the meaning of the word is 'smeared' over the whole vector with different numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec &mdash; Set of algorithms developed by Tomas Mikolov in 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These algorithms build dense vector representations for words, as outlined above.\n",
    "* Became highly influential in statistical NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a model that aims to predict between a center word $w_t$ and context words in terms of word vectors:\n",
    "    * $P(context \\mid w_{t}) = \\ldots$\n",
    "    * Which has a loss function, eg.\n",
    "    * $J = 1 - P(w_{-t} \\mid w_{t})$ -- $w_{-t}$ refers to everything not $w_t$, hence all the words in the context\n",
    "    * We look at many positions $t$ in a big language corpus.\n",
    "    * We keep adjusting the vector representations of words to minimize this loss.\n",
    "    * As a result, we obtain dense word vectors that are very powerful in representing meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two algorithms of Word2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Skip-Gram (SG)__ &mdash; predicts context words given target\n",
    "* __Continuous Bag of Words (CBOW)__ &mdash; predicts target word from bag-of-words context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two training methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Hierarchical softmax__\n",
    "* __Negative sampling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SkipGramPrediction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each word $t = 1 \\ldots T$, predict surrounding words in a window of 'radius' $m$ of every word.\n",
    "* Objective function: maximize the probability of any context word given the current center word.\n",
    "\n",
    "\n",
    "* $\\displaystyle J'(\\theta) = \\prod_{t=1}^T \\prod_{\\substack{-m \\leq j \\leq m\\\\ j \\neq 0}} P(w_{t+j} \\mid w_t ; \\theta)$\n",
    "* __Negative Log-Likelihood:__ $\\displaystyle J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^T \\sum_{\\substack{-m \\leq j \\leq m\\\\ j \\neq 0}} \\log P(w_{t+j} \\mid w_t)$\n",
    "    * Where $\\theta$ represents all variables we will optimize (the vector representations of the words)\n",
    "    * Notes on second equation:\n",
    "        * $-$ changes it to a minimization problem, since machine learning people like to minimize things, rather than maximizing them\n",
    "        * $\\frac{1}{T}$ &mdash; normalization, making it 'per word', instead of a probability for the whole corpus\n",
    "        * $\\log$ &mdash; our products turn into sums, making the math a lot easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __How is the probability itself measured?__\n",
    "\n",
    "\n",
    "* For $P(w_{t+j} \\mid w_t)$:\n",
    "    * $P(o \\mid c) = \\frac{exp(u_{o}^T v_c)}{\\sum_{w=1}^v exp(u_{w}^T v_c)}$ &mdash; _softmax_ form\n",
    "\n",
    "\n",
    "* $o$: the outside/output word index\n",
    "* $c$: the center word index\n",
    "* $v_c$ and $u_o$: \"center\" and \"outside\" vectors of indices $c$ and $o$\n",
    "\n",
    "\n",
    "* We put the dot product of the two vectors into a softmax form.\n",
    "* The dot product will be bigger if $u$ and $v$ are more similar.\n",
    "* Softmax form: standard way to turn numbers into a probability distribution.\n",
    "\n",
    "    * $P_i = \\frac{e^{u_i}}{\\sum_{j}e^{u_j}}$\n",
    "\n",
    "\n",
    "* Exponentiation makes sure everything is positive.\n",
    "* Dividing by the sum of all the numbers gives a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As a result of this function, each word has two representations: one for when it is the context word, another for when it is the center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final picture of the Skip-Gram model:\n",
    "\n",
    "\n",
    "<img src=\"SkipGramDiagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{\\theta} = \\left[\\begin{array}\n",
    "{r}\n",
    "v_{aardvark} \\\\\n",
    "v_{a} \\\\\n",
    "\\vdots \\\\\n",
    "u_{aardvark} \\\\\n",
    "u_{a} \\\\\n",
    "\\vdots \\\\\n",
    "u_{zebra}\n",
    "\\end{array}\\right]\n",
    "\\in {\\rm I\\!R}^{2dV}\n",
    "$$\n",
    "\n",
    "* $V$ &mdash; size of vocabulary\n",
    "* $d$ &mdash; dimensionality of vector representations\n",
    "* To train, compute the gradient for all vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Result\n",
    "\n",
    "* The final word vectors obtained are able to capture some general and quite useful semantic information about words and their relationship to one another.\n",
    "* In the latent vector space, different directions specialize towards different semantic and even grammatical relationships (male-female, verb tense, country-capital, and so on).\n",
    "* This can be seen in the following graphs, using t-SNE for dimensionality reduction (from https://www.tensorflow.org/tutorials/word2vec):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"W2VSemanticRelationships.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the following is a t-SNE visualization of learned Word2Vec embeddings, showing that similar words do indeed cluster nearby each other:\n",
    "\n",
    "<img src=\"W2VVisualization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim, logging, multiprocessing\n",
    "import numpy as np\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Configuring the word2vec model\n",
    "w2vconfig = {\n",
    "    'min_count': 6, # minimum frequency of a word to be included in the model\n",
    "    'hs': 0, # training method: 0 --- negative sampling, 1 --- hierarchical softmax\n",
    "    'negative': 5, # number of \"negative\" words selected to update model weights\n",
    "    'size': 100, # dimension of the vectors\n",
    "    'sg': 0, # 0 --- CBOW, 1 --- skip-gram \n",
    "    'batch_words': 10000, # batch size for training the model\n",
    "    'iter': 20, # number of epochs\n",
    "    'window': 5, # context window\n",
    "    'workers': multiprocessing.cpu_count(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initializing the word2vec model using the above configuration\n",
    "model = gensim.models.Word2Vec(**w2vconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preparing the word2vec input (the gensim word2vec module expects a sequence of sentences as its input (list of lists))\n",
    "# Eg. sentences = [['sentence', 'one'], ['sentence', 'two'], ...]\n",
    "# Prior to making the model, helps to pre-process data (tokenization, stemming, POS-tagging, stop word/punctuation removal, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Here the brown corpus will be used as a demonstration\n",
    "from nltk.corpus import brown\n",
    "sentences = brown.sents(categories=brown.categories())\n",
    "print(len(sentences))\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-04 14:57:40,404 : INFO : collecting all words and their counts\n",
      "2018-02-04 14:57:40,407 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-04 14:57:41,129 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
      "2018-02-04 14:57:41,881 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
      "2018-02-04 14:57:42,526 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
      "2018-02-04 14:57:43,181 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
      "2018-02-04 14:57:43,747 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
      "2018-02-04 14:57:44,229 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
      "2018-02-04 14:57:44,231 : INFO : Loading a fresh vocabulary\n",
      "2018-02-04 14:57:44,383 : INFO : min_count=6 retains 13160 unique words (23% of original 56057, drops 42897)\n",
      "2018-02-04 14:57:44,387 : INFO : min_count=6 leaves 1085021 word corpus (93% of original 1161192, drops 76171)\n",
      "2018-02-04 14:57:44,455 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2018-02-04 14:57:44,470 : INFO : sample=0.001 downsamples 42 most-common words\n",
      "2018-02-04 14:57:44,474 : INFO : downsampling leaves estimated 770495 word corpus (71.0% of prior 1085021)\n",
      "2018-02-04 14:57:44,476 : INFO : estimated required memory for 13160 words and 100 dimensions: 17108000 bytes\n",
      "2018-02-04 14:57:44,564 : INFO : resetting layer weights\n",
      "2018-02-04 14:57:44,814 : INFO : training model with 4 workers on 13160 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-04 14:57:45,838 : INFO : PROGRESS: at 0.95% examples, 154968 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:46,846 : INFO : PROGRESS: at 2.03% examples, 165505 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:47,855 : INFO : PROGRESS: at 3.09% examples, 174300 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:48,881 : INFO : PROGRESS: at 4.62% examples, 177789 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:49,884 : INFO : PROGRESS: at 5.76% examples, 176449 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:50,883 : INFO : PROGRESS: at 6.62% examples, 170257 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:51,904 : INFO : PROGRESS: at 7.61% examples, 170841 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:52,910 : INFO : PROGRESS: at 8.47% examples, 167665 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:53,935 : INFO : PROGRESS: at 9.71% examples, 165027 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:54,943 : INFO : PROGRESS: at 10.85% examples, 166069 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:55,952 : INFO : PROGRESS: at 12.02% examples, 168205 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:56,960 : INFO : PROGRESS: at 13.11% examples, 170711 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:57,997 : INFO : PROGRESS: at 14.43% examples, 169912 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:57:59,012 : INFO : PROGRESS: at 15.62% examples, 169905 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:00,018 : INFO : PROGRESS: at 16.75% examples, 170785 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:01,049 : INFO : PROGRESS: at 17.59% examples, 169279 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:02,092 : INFO : PROGRESS: at 18.33% examples, 166767 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:03,144 : INFO : PROGRESS: at 19.53% examples, 164862 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:04,158 : INFO : PROGRESS: at 20.57% examples, 164082 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:05,170 : INFO : PROGRESS: at 21.63% examples, 164350 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:06,199 : INFO : PROGRESS: at 22.58% examples, 164442 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:07,225 : INFO : PROGRESS: at 23.80% examples, 165547 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:08,291 : INFO : PROGRESS: at 25.02% examples, 164312 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:09,325 : INFO : PROGRESS: at 25.83% examples, 162720 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:10,358 : INFO : PROGRESS: at 26.88% examples, 162868 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:11,388 : INFO : PROGRESS: at 27.87% examples, 163265 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:12,401 : INFO : PROGRESS: at 29.09% examples, 163577 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:13,434 : INFO : PROGRESS: at 30.37% examples, 163678 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:14,464 : INFO : PROGRESS: at 31.36% examples, 163345 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:15,486 : INFO : PROGRESS: at 32.36% examples, 163663 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:16,506 : INFO : PROGRESS: at 33.34% examples, 163894 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:17,520 : INFO : PROGRESS: at 34.63% examples, 163516 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:18,590 : INFO : PROGRESS: at 35.70% examples, 163026 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:19,607 : INFO : PROGRESS: at 36.74% examples, 163172 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:20,619 : INFO : PROGRESS: at 37.85% examples, 164093 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:21,629 : INFO : PROGRESS: at 39.05% examples, 164314 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:22,641 : INFO : PROGRESS: at 40.39% examples, 164646 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:23,660 : INFO : PROGRESS: at 41.50% examples, 164920 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:24,669 : INFO : PROGRESS: at 42.61% examples, 165702 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:25,679 : INFO : PROGRESS: at 43.86% examples, 166335 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:26,692 : INFO : PROGRESS: at 45.22% examples, 166444 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:27,729 : INFO : PROGRESS: at 46.37% examples, 166732 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:28,730 : INFO : PROGRESS: at 47.34% examples, 166806 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:29,769 : INFO : PROGRESS: at 48.46% examples, 167269 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:30,796 : INFO : PROGRESS: at 50.01% examples, 167611 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:31,806 : INFO : PROGRESS: at 51.19% examples, 168057 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:32,822 : INFO : PROGRESS: at 52.40% examples, 168879 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:33,826 : INFO : PROGRESS: at 53.61% examples, 169509 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:34,833 : INFO : PROGRESS: at 55.08% examples, 169722 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:35,851 : INFO : PROGRESS: at 56.26% examples, 170064 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:36,876 : INFO : PROGRESS: at 57.43% examples, 170626 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:37,891 : INFO : PROGRESS: at 58.59% examples, 171009 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:38,914 : INFO : PROGRESS: at 60.10% examples, 171246 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:39,934 : INFO : PROGRESS: at 61.25% examples, 171409 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:40,961 : INFO : PROGRESS: at 62.31% examples, 171551 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:41,990 : INFO : PROGRESS: at 63.12% examples, 171052 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:43,016 : INFO : PROGRESS: at 64.50% examples, 171008 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:44,028 : INFO : PROGRESS: at 65.74% examples, 171215 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:45,036 : INFO : PROGRESS: at 66.90% examples, 171520 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:46,044 : INFO : PROGRESS: at 68.00% examples, 171951 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:47,046 : INFO : PROGRESS: at 69.48% examples, 172294 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:48,056 : INFO : PROGRESS: at 70.73% examples, 172465 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:49,076 : INFO : PROGRESS: at 72.00% examples, 173004 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:50,092 : INFO : PROGRESS: at 73.06% examples, 173269 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-04 14:58:51,106 : INFO : PROGRESS: at 74.52% examples, 173429 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:52,108 : INFO : PROGRESS: at 75.72% examples, 173498 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:53,134 : INFO : PROGRESS: at 76.92% examples, 173782 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:54,145 : INFO : PROGRESS: at 78.05% examples, 174222 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:55,177 : INFO : PROGRESS: at 79.50% examples, 174319 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:56,186 : INFO : PROGRESS: at 80.75% examples, 174443 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:57,203 : INFO : PROGRESS: at 81.95% examples, 174721 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:58,239 : INFO : PROGRESS: at 83.08% examples, 175070 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:58:59,240 : INFO : PROGRESS: at 84.43% examples, 175040 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:00,266 : INFO : PROGRESS: at 85.74% examples, 175203 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:01,294 : INFO : PROGRESS: at 86.94% examples, 175429 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:02,330 : INFO : PROGRESS: at 88.07% examples, 175749 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:03,354 : INFO : PROGRESS: at 89.53% examples, 175830 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:04,386 : INFO : PROGRESS: at 90.80% examples, 175948 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:05,409 : INFO : PROGRESS: at 92.00% examples, 176168 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:06,413 : INFO : PROGRESS: at 93.16% examples, 176614 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:07,419 : INFO : PROGRESS: at 94.66% examples, 176722 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:08,436 : INFO : PROGRESS: at 95.83% examples, 176690 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:09,444 : INFO : PROGRESS: at 97.06% examples, 177006 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:10,473 : INFO : PROGRESS: at 98.27% examples, 177440 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:11,507 : INFO : PROGRESS: at 99.77% examples, 177409 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-04 14:59:11,671 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-04 14:59:11,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-04 14:59:11,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-04 14:59:11,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-04 14:59:11,684 : INFO : training on 23223840 raw words (15409703 effective words) took 86.9s, 177401 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15409703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the vocabulary and training the model\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'person': [('child', 0.7117858529090881), ('man', 0.6841299533843994), ('woman', 0.6629840135574341), ('difficulty', 0.6074833869934082), ('teacher', 0.6068326830863953), ('patient', 0.602636456489563), ('artist', 0.5924439430236816), ('dancer', 0.589277446269989), ('word', 0.588111400604248), ('case', 0.5839000940322876)] \n",
      "\n",
      "Most similar words to 'Asia': [('Africa', 0.799575686454773), ('East', 0.772148847579956), ('South', 0.7577406167984009), ('Viet', 0.7552622556686401), ('Europe', 0.7377159595489502), ('Nam', 0.7342245578765869), ('North', 0.7217272520065308), ('Southeast', 0.7211916446685791), ('France', 0.714782178401947), ('Middle', 0.7034892439842224)] \n",
      "\n",
      "Most similar words to 'two': [('three', 0.8228723406791687), ('four', 0.7181538343429565), ('several', 0.6572548151016235), ('six', 0.6392922401428223), ('five', 0.6161919236183167), ('few', 0.5737988948822021), ('eight', 0.5627668499946594), ('Two', 0.5545819997787476), ('ten', 0.5273005962371826), ('seven', 0.5187063217163086)] \n",
      "\n",
      "Most similar words to 'Fred': [('Dave', 0.8407467603683472), ('Warren', 0.7809328436851501), ('Jones', 0.757568359375), ('Burton', 0.7513214945793152), ('Colonel', 0.7505892515182495), ('Peter', 0.7305557727813721), ('McClellan', 0.72906494140625), ('Russell', 0.7285399436950684), ('Lee', 0.7283788323402405), ('Richards', 0.7268639802932739)] \n",
      "\n",
      "'France' - 'Paris' + 'Germany' (expected 'Berlin'): [('East', 0.6897165179252625), ('Communist', 0.6726321578025818), ('Asia', 0.6704003810882568), ('Africa', 0.6683363914489746), ('Europe', 0.6552317142486572), ('Viet', 0.638154149055481), ('West', 0.6336057186126709), ('South', 0.6326110363006592), ('Berlin', 0.6251089572906494), ('China', 0.619153618812561)] \n",
      "\n",
      "Similarity of 'two' and 'three': 0.822872345513 \n",
      "\n",
      "Similarity of 'go' and 'come': 0.702878176032 \n",
      "\n",
      "Similarity of 'go' and 'orange': -0.105405191925\n"
     ]
    }
   ],
   "source": [
    "## Testing the model\n",
    "\n",
    "print(\"Most similar words to 'person':\", model.most_similar(positive=[\"person\"]), \"\\n\")\n",
    "print(\"Most similar words to 'Asia':\", model.most_similar(positive=[\"Asia\"]), \"\\n\")\n",
    "print(\"Most similar words to 'two':\", model.most_similar(positive=[\"two\"]), \"\\n\")\n",
    "print(\"Most similar words to 'Fred':\", model.most_similar(positive=[\"Fred\"]), \"\\n\")\n",
    "print(\"'France' - 'Paris' + 'Germany' (expected 'Berlin'):\", model.most_similar(positive=[\"France\", \"Germany\"], negative=[\"Paris\"]), \"\\n\")\n",
    "\n",
    "# Similar words:\n",
    "print(\"Similarity of 'two' and 'three':\", model.similarity(\"two\", \"three\"), \"\\n\")\n",
    "print(\"Similarity of 'go' and 'come':\", model.similarity(\"go\", \"come\"), \"\\n\")\n",
    "\n",
    "# Dissimilar words:\n",
    "print(\"Similarity of 'go' and 'orange':\", model.similarity(\"go\", \"orange\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.56311476e-01   2.64052898e-01  -4.40935969e-01   9.09542441e-02\n",
      "   1.93358675e-01  -3.28421444e-02  -7.56693408e-02  -8.06089044e-02\n",
      "   4.26417142e-01   2.46043742e-01   4.28434104e-01   6.42067716e-02\n",
      "  -1.90532133e-01  -2.35424474e-01  -2.47063249e-01  -7.06442177e-01\n",
      "  -7.97080100e-02   6.57491386e-04  -3.12289327e-01  -2.54352927e-01\n",
      "  -2.36614943e-01  -2.16081217e-01  -5.42365670e-01   3.05294245e-01\n",
      "   1.70133665e-01  -2.63780534e-01   2.84948409e-01  -1.43995166e-01\n",
      "   6.79376185e-01   3.22120003e-02   1.55920431e-01   2.93974906e-01\n",
      "   1.10957816e-01  -2.55263507e-01  -3.12969148e-01  -4.03587192e-01\n",
      "  -1.78760335e-01  -4.75222152e-03  -9.69010964e-02  -3.63178775e-02\n",
      "  -1.15098897e-02  -2.23732904e-01  -4.28943157e-01   3.22065026e-01\n",
      "   2.34043315e-01  -2.73598254e-01   8.57908744e-03   3.94227475e-01\n",
      "  -2.97977962e-02  -1.71636477e-01   2.86382198e-01   5.98386228e-01\n",
      "  -2.14478672e-01   8.30935389e-02  -3.59393835e-01  -1.28186807e-01\n",
      "   1.67623907e-02   1.39775559e-01   9.24903378e-02  -4.81036842e-01\n",
      "   5.00908136e-01   2.86254995e-02   1.20244667e-01  -3.10627520e-01\n",
      "  -4.78256159e-02   1.61170915e-01   1.19893551e-01   2.68582970e-01\n",
      "   4.79694933e-01  -4.76536036e-01   4.47065243e-03   4.04793680e-01\n",
      "  -3.96741420e-01  -9.24631022e-03   4.34379816e-01  -8.97509381e-02\n",
      "   2.69082755e-01   9.19040665e-02   4.66915429e-01   2.78500617e-01\n",
      "  -4.38256375e-02  -1.73467979e-01  -7.65364096e-02   3.74243647e-01\n",
      "   1.85136959e-01   1.39147257e-02  -2.99212933e-01   1.28197774e-01\n",
      "  -4.43183742e-02   3.94718558e-01   2.08122641e-01  -2.01303847e-02\n",
      "  -1.50997281e-01  -2.89756775e-01   8.76634568e-03   1.02300972e-01\n",
      "   1.84189141e-01  -5.79449274e-02   8.33248124e-02  -2.04305425e-01]\n"
     ]
    }
   ],
   "source": [
    "# Checking individual vectors\n",
    "orange_vector = model.wv.syn0[model.wv.vocab[\"orange\"].index]\n",
    "print(orange_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternatively, can use a pre-trained word2vec model.\n",
    "* W2V model made by Google (1.5 GB, vocabulary of 3 million words, 100 billion words total): https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "* W2V models for many different languages: https://github.com/Kyubyong/wordvectors\n",
    "* More W2V models: https://sites.google.com/site/rmyeid/projects/polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example: loading the above word2vec model by Google\n",
    "* large_model = gensim.models.KeyedVectors.load_word2vec_format('directory/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
