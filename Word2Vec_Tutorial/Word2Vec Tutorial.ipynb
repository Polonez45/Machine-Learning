{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word2Vec Tutorial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Based on Lecture 2 from the Stanford Deep Learning Series by Chris Manning__\n",
    "* __Word2Vec: Set of algorithms developed by Mikolov et al. (2013)__\n",
    "* __Original Word2Vec Paper: https://arxiv.org/abs/1301.3781__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How do we represent the meaning of a word computationally?\n",
    "* Often we make use of a taxonomy like WordNet, using hypernym relationships and synonym sets.\n",
    "* Problem with this:\n",
    "    * Misses nuances (eg. 'good' and 'expert' are considered synonyms in WordNet, but \"he's an expert at deep learning\" and \"he's good at deep learning\" clearly have different meanings).\n",
    "    * Misses new words (must constantly keep up to date), requires a lot of human labor.\n",
    "    \n",
    "    \n",
    "* This can be considered a 'discrete' representation, using atomic symbols.\n",
    "* Words like 'hotel', 'conference', 'walk', in vector space terms are represented using a vector of one 1 and many zeros [0 0 0 0 0 0 0 0 1 0 0 0], thus a 'one-hot' vector representation.\n",
    "* These vectors end up being very long, depending on the vocabulary.\n",
    "* Another problem: doesn't show any inherent relationship between words, eg. 'Seattle motel' and 'Seattle hotel' should be considered similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better approach:\n",
    "* Find a way for the vectors to directly encode similarity &mdash; distributional similarity (the meaning of a word is the context it is in).\n",
    "* \"You shall know a word by the company it keeps.\" J. R. Firth 1957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Our goal:__ build a dense vector for each word, chosen so that it is good at predicting other words appearing in its context &mdash; the other words also represented by vectors of their own.\n",
    "* Eg. linguistics = [0.286 0.792 -0.177 -0.107 0.109 -0.542 0.349 0.271]\n",
    "* This type of dense vector representation is distributed &mdash; instead of an atomic 'one-hot' vector representation, the meaning of the word is 'smeared' over the whole vector with different numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec &mdash; Set of algorithms developed by Tomas Mikolov in 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These algorithms build dense vector representations for words, as outlined above.\n",
    "* Became highly influential in statistical NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a model that aims to predict between a center word $w_t$ and context words in terms of word vectors:\n",
    "    * $P(context \\mid w_{t}) = \\ldots$\n",
    "    * Which has a loss function, eg.\n",
    "    * $J = 1 - P(w_{-t} \\mid w_{t})$ -- $w_{-t}$ refers to everything not $w_t$, hence all the words in the context\n",
    "    * We look at many positions $t$ in a big language corpus.\n",
    "    * We keep adjusting the vector representations of words to minimize this loss.\n",
    "    * As a result, we obtain dense word vectors that are very powerful in representing meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two algorithms of Word2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Skip-Gram (SG)__ &mdash; predicts context words given target\n",
    "* __Continuous Bag of Words (CBOW)__ &mdash; predicts target word from bag-of-words context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two training methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Hierarchical softmax__\n",
    "* __Negative sampling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SkipGramPrediction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each word $t = 1 \\ldots T$, predict surrounding words in a window of 'radius' $m$ of every word.\n",
    "* Objective function: maximize the probability of any context word given the current center word.\n",
    "\n",
    "\n",
    "* $\\displaystyle J'(\\theta) = \\prod_{t=1}^T \\prod_{\\substack{-m \\leq j \\leq m\\\\ j \\neq 0}} P(w_{t+j} \\mid w_t ; \\theta)$\n",
    "* __Negative Log-Likelihood:__ $\\displaystyle J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^T \\sum_{\\substack{-m \\leq j \\leq m\\\\ j \\neq 0}} \\log P(w_{t+j} \\mid w_t)$\n",
    "    * Where $\\theta$ represents all variables we will optimize (the vector representations of the words)\n",
    "    * Notes on second equation:\n",
    "        * $-$ changes it to a minimization problem, since machine learning people like to minimize things, rather than maximizing them\n",
    "        * $\\frac{1}{T}$ &mdash; normalization, making it 'per word', instead of a probability for the whole corpus\n",
    "        * $\\log$ &mdash; our products turn into sums, making the math a lot easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __How is the probability itself measured?__\n",
    "\n",
    "\n",
    "* For $P(w_{t+j} \\mid w_t)$:\n",
    "    * $P(o \\mid c) = \\frac{exp(u_{o}^T v_c)}{\\sum_{w=1}^v exp(u_{w}^T v_c)}$ &mdash; _softmax_ form\n",
    "\n",
    "\n",
    "* $o$: the outside/output word index\n",
    "* $c$: the center word index\n",
    "* $v_c$ and $u_o$: \"center\" and \"outside\" vectors of indices $c$ and $o$\n",
    "\n",
    "\n",
    "* We put the dot product of the two vectors into a softmax form.\n",
    "* The dot product will be bigger if $u$ and $v$ are more similar.\n",
    "* Softmax form: standard way to turn numbers into a probability distribution.\n",
    "\n",
    "    * $P_i = \\frac{e^{u_i}}{\\sum_{j}e^{u_j}}$\n",
    "\n",
    "\n",
    "* Exponentiation makes sure everything is positive.\n",
    "* Dividing by the sum of all the numbers gives a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As a result of this function, each word has two representations: one for when it is the context word, another for when it is the center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final picture of the Skip-Gram model:\n",
    "\n",
    "\n",
    "<img src=\"SkipGramDiagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{\\theta} = \\left[\\begin{array}\n",
    "{r}\n",
    "v_{aardvark} \\\\\n",
    "v_{a} \\\\\n",
    "\\vdots \\\\\n",
    "u_{aardvark} \\\\\n",
    "u_{a} \\\\\n",
    "\\vdots \\\\\n",
    "u_{zebra}\n",
    "\\end{array}\\right]\n",
    "\\in {\\rm I\\!R}^{2dV}\n",
    "$$\n",
    "\n",
    "* $V$ &mdash; size of vocabulary\n",
    "* $d$ &mdash; dimensionality of vector representations\n",
    "* To train, compute the gradient for all vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Result\n",
    "\n",
    "* The final word vectors obtained are able to capture some general and quite useful semantic information about words and their relationship to one another.\n",
    "* In the latent vector space, different directions specialize towards different semantic and even grammatical relationships (male-female, verb tense, country-capital, and so on).\n",
    "* This can be seen in the following graphs, using t-SNE for dimensionality reduction (from https://www.tensorflow.org/tutorials/word2vec):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"W2VSemanticRelationships.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the following is a t-SNE visualization of learned Word2Vec embeddings, showing that similar words do indeed cluster nearby each other:\n",
    "\n",
    "<img src=\"W2VVisualization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tigam\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging, multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Configuring the word2vec model\n",
    "w2vconfig = {\n",
    "    'min_count': 6, # minimum frequency of a word to be included in the model\n",
    "    'hs': 0, # training method: 0 --- negative sampling, 1 --- hierarchical softmax\n",
    "    'negative': 5, # number of \"negative\" words selected to update model weights\n",
    "    'size': 100, # dimension of the vectors\n",
    "    'sg': 0, # 0 --- CBOW, 1 --- skip-gram \n",
    "    'batch_words': 10000, # batch size for training the model\n",
    "    'iter': 20, # number of epochs\n",
    "    'window': 5, # context window\n",
    "    'workers': multiprocessing.cpu_count(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initializing the word2vec model using the above configuration\n",
    "model = gensim.models.Word2Vec(**w2vconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preparing the word2vec input (the gensim word2vec module expects a sequence of sentences as its input (list of lists))\n",
    "# Eg. sentences = [['sentence', 'one'], ['sentence', 'two'], ...]\n",
    "# Prior to making the model, helps to pre-process data (tokenization, stemming, POS-tagging, stop word/punctuation removal, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Here the brown corpus will be used as a demonstration\n",
    "from nltk.corpus import brown\n",
    "sentences = brown.sents(categories=brown.categories())\n",
    "print(len(sentences))\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 16:22:40,243 : INFO : collecting all words and their counts\n",
      "2018-02-03 16:22:40,247 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 16:22:40,952 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
      "2018-02-03 16:22:41,562 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
      "2018-02-03 16:22:42,306 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
      "2018-02-03 16:22:42,944 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
      "2018-02-03 16:22:43,438 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
      "2018-02-03 16:22:43,882 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
      "2018-02-03 16:22:43,883 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 16:22:44,067 : INFO : min_count=6 retains 13160 unique words (23% of original 56057, drops 42897)\n",
      "2018-02-03 16:22:44,068 : INFO : min_count=6 leaves 1085021 word corpus (93% of original 1161192, drops 76171)\n",
      "2018-02-03 16:22:44,146 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2018-02-03 16:22:44,155 : INFO : sample=0.001 downsamples 42 most-common words\n",
      "2018-02-03 16:22:44,162 : INFO : downsampling leaves estimated 770495 word corpus (71.0% of prior 1085021)\n",
      "2018-02-03 16:22:44,167 : INFO : estimated required memory for 13160 words and 100 dimensions: 17108000 bytes\n",
      "2018-02-03 16:22:44,253 : INFO : resetting layer weights\n",
      "2018-02-03 16:22:44,471 : INFO : training model with 4 workers on 13160 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 16:22:45,495 : INFO : PROGRESS: at 0.95% examples, 155110 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:46,512 : INFO : PROGRESS: at 2.11% examples, 171460 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:47,542 : INFO : PROGRESS: at 2.77% examples, 152755 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:48,549 : INFO : PROGRESS: at 3.93% examples, 157612 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:49,586 : INFO : PROGRESS: at 5.23% examples, 158157 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:50,600 : INFO : PROGRESS: at 6.26% examples, 158842 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:51,614 : INFO : PROGRESS: at 7.28% examples, 161153 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:52,685 : INFO : PROGRESS: at 8.36% examples, 163690 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:53,692 : INFO : PROGRESS: at 9.65% examples, 162518 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:54,700 : INFO : PROGRESS: at 10.67% examples, 161332 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:55,725 : INFO : PROGRESS: at 11.68% examples, 161278 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:56,727 : INFO : PROGRESS: at 12.67% examples, 162630 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:57,769 : INFO : PROGRESS: at 13.84% examples, 163416 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:58,797 : INFO : PROGRESS: at 15.01% examples, 161496 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:22:59,798 : INFO : PROGRESS: at 16.15% examples, 162954 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:00,825 : INFO : PROGRESS: at 17.36% examples, 165557 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:01,835 : INFO : PROGRESS: at 18.38% examples, 166335 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:02,848 : INFO : PROGRESS: at 19.79% examples, 166292 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:03,851 : INFO : PROGRESS: at 20.80% examples, 165784 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:04,861 : INFO : PROGRESS: at 21.81% examples, 165694 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:05,877 : INFO : PROGRESS: at 22.80% examples, 166148 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:06,891 : INFO : PROGRESS: at 24.10% examples, 167002 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:07,908 : INFO : PROGRESS: at 25.27% examples, 166264 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:08,923 : INFO : PROGRESS: at 26.37% examples, 166658 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:09,925 : INFO : PROGRESS: at 27.53% examples, 168103 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:10,936 : INFO : PROGRESS: at 28.85% examples, 169505 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:11,960 : INFO : PROGRESS: at 30.18% examples, 169241 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:12,975 : INFO : PROGRESS: at 31.41% examples, 170154 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:13,998 : INFO : PROGRESS: at 32.59% examples, 171404 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:15,014 : INFO : PROGRESS: at 33.95% examples, 172472 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:16,030 : INFO : PROGRESS: at 35.36% examples, 172805 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:17,039 : INFO : PROGRESS: at 36.59% examples, 173536 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:18,045 : INFO : PROGRESS: at 37.62% examples, 173825 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:19,067 : INFO : PROGRESS: at 39.00% examples, 174668 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:20,092 : INFO : PROGRESS: at 40.27% examples, 174301 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:21,110 : INFO : PROGRESS: at 41.30% examples, 173956 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:22,135 : INFO : PROGRESS: at 42.28% examples, 173750 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:23,139 : INFO : PROGRESS: at 43.48% examples, 174574 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:24,163 : INFO : PROGRESS: at 45.06% examples, 174966 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:25,183 : INFO : PROGRESS: at 46.25% examples, 175259 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:26,185 : INFO : PROGRESS: at 47.31% examples, 175460 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:27,214 : INFO : PROGRESS: at 48.35% examples, 175623 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:28,229 : INFO : PROGRESS: at 49.64% examples, 175068 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:29,275 : INFO : PROGRESS: at 50.58% examples, 174076 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:30,276 : INFO : PROGRESS: at 51.71% examples, 174298 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:31,291 : INFO : PROGRESS: at 52.89% examples, 175039 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:32,325 : INFO : PROGRESS: at 54.37% examples, 175451 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:33,352 : INFO : PROGRESS: at 55.53% examples, 175148 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:34,363 : INFO : PROGRESS: at 56.70% examples, 175429 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:35,381 : INFO : PROGRESS: at 57.88% examples, 176071 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:36,398 : INFO : PROGRESS: at 58.99% examples, 175715 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:37,433 : INFO : PROGRESS: at 60.27% examples, 175412 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:38,442 : INFO : PROGRESS: at 61.21% examples, 174942 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:39,456 : INFO : PROGRESS: at 62.42% examples, 175541 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:40,468 : INFO : PROGRESS: at 63.68% examples, 176066 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:41,533 : INFO : PROGRESS: at 64.88% examples, 175260 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:42,546 : INFO : PROGRESS: at 65.63% examples, 174248 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:43,588 : INFO : PROGRESS: at 66.15% examples, 172609 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:44,591 : INFO : PROGRESS: at 66.90% examples, 171815 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:45,751 : INFO : PROGRESS: at 67.82% examples, 171263 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:46,843 : INFO : PROGRESS: at 68.83% examples, 170729 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:47,848 : INFO : PROGRESS: at 70.08% examples, 170451 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:48,864 : INFO : PROGRESS: at 71.27% examples, 170718 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:49,895 : INFO : PROGRESS: at 72.44% examples, 171137 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 16:23:50,918 : INFO : PROGRESS: at 73.70% examples, 171610 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:51,925 : INFO : PROGRESS: at 75.19% examples, 171822 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:52,949 : INFO : PROGRESS: at 76.43% examples, 172145 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:53,971 : INFO : PROGRESS: at 77.43% examples, 172155 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:54,987 : INFO : PROGRESS: at 78.22% examples, 171767 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:56,014 : INFO : PROGRESS: at 79.50% examples, 171447 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:57,035 : INFO : PROGRESS: at 80.71% examples, 171495 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:58,047 : INFO : PROGRESS: at 81.65% examples, 171194 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:23:59,063 : INFO : PROGRESS: at 82.60% examples, 171167 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:00,075 : INFO : PROGRESS: at 83.84% examples, 171428 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:01,086 : INFO : PROGRESS: at 85.29% examples, 171594 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:02,104 : INFO : PROGRESS: at 86.52% examples, 171903 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:03,118 : INFO : PROGRESS: at 87.70% examples, 172370 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:04,152 : INFO : PROGRESS: at 89.12% examples, 172728 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:05,187 : INFO : PROGRESS: at 90.53% examples, 172896 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:06,205 : INFO : PROGRESS: at 91.77% examples, 173238 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:07,212 : INFO : PROGRESS: at 92.95% examples, 173687 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:08,220 : INFO : PROGRESS: at 94.40% examples, 173911 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:09,266 : INFO : PROGRESS: at 95.47% examples, 173556 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:10,291 : INFO : PROGRESS: at 96.69% examples, 173789 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:11,292 : INFO : PROGRESS: at 97.75% examples, 173992 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:12,325 : INFO : PROGRESS: at 99.15% examples, 174227 words/s, in_qsize 0, out_qsize 0\n",
      "2018-02-03 16:24:12,882 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-03 16:24:12,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 16:24:12,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 16:24:12,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 16:24:12,898 : INFO : training on 23223840 raw words (15409703 effective words) took 88.4s, 174280 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15409703"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the vocabulary and training the model\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 16:27:59,492 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'person': [('child', 0.715959906578064), ('man', 0.6727136373519897), ('woman', 0.6627339124679565), ('difficulty', 0.616508960723877), ('patient', 0.612080454826355), ('teacher', 0.6093043684959412), ('artist', 0.5953537225723267), ('word', 0.5873693227767944), ('case', 0.5868275761604309), ('dancer', 0.5713365077972412)] \n",
      "\n",
      "Most similar words to 'Asia': [('Africa', 0.8071120977401733), ('Southeast', 0.7689347863197327), ('East', 0.7660007476806641), ('South', 0.7628188133239746), ('Viet', 0.7604211568832397), ('Europe', 0.7526821494102478), ('Nam', 0.7451930046081543), ('western', 0.7263959646224976), ('North', 0.7213359475135803), ('France', 0.7085673809051514)] \n",
      "\n",
      "Most similar words to 'two': [('three', 0.8061007261276245), ('four', 0.715324342250824), ('several', 0.6597403287887573), ('six', 0.6394720077514648), ('five', 0.6207407712936401), ('few', 0.5648008584976196), ('Two', 0.5595917701721191), ('eight', 0.5593658685684204), ('seven', 0.520887017250061), ('couple', 0.5183953046798706)] \n",
      "\n",
      "Most similar words to 'Fred': [('Dave', 0.8448946475982666), ('Warren', 0.7692791223526001), ('Burton', 0.750530481338501), ('Jones', 0.7470552325248718), ('Richards', 0.7447143793106079), ('Colonel', 0.7438591718673706), ('Lou', 0.7300326824188232), ('Lee', 0.7250141501426697), ('Peter', 0.7214818000793457), ('Stevie', 0.7163942456245422)] \n",
      "\n",
      "'England' - 'London' + 'Germany' (expected 'Berlin'): [('Africa', 0.5935020446777344), ('Europe', 0.5778156518936157), ('South', 0.5616999864578247), ('Communist', 0.5559509992599487), ('European', 0.5503194332122803), ('Soviet', 0.5308911800384521), ('Asia', 0.5276586413383484), ('Western', 0.5222490429878235), ('nations', 0.5205241441726685), ('peoples', 0.5178728103637695)] \n",
      "\n",
      "Similarity of 'two' and 'three': 0.806100676582 \n",
      "\n",
      "Similarity of 'go' and 'come': 0.69895709975 \n",
      "\n",
      "Similarity of 'go' and 'orange': -0.104711572377\n"
     ]
    }
   ],
   "source": [
    "## Testing the model\n",
    "\n",
    "print(\"Most similar words to 'person':\", model.most_similar(positive=[\"person\"]), \"\\n\")\n",
    "print(\"Most similar words to 'Asia':\", model.most_similar(positive=[\"Asia\"]), \"\\n\")\n",
    "print(\"Most similar words to 'two':\", model.most_similar(positive=[\"two\"]), \"\\n\")\n",
    "print(\"Most similar words to 'Fred':\", model.most_similar(positive=[\"Fred\"]), \"\\n\")\n",
    "print(\"'England' - 'London' + 'Germany' (expected 'Berlin'):\", model.most_similar(positive=[\"England\", \"Germany\"], negative=[\"London\"]), \"\\n\")\n",
    "\n",
    "# Similar words:\n",
    "print(\"Similarity of 'two' and 'three':\", model.similarity(\"two\", \"three\"), \"\\n\")\n",
    "print(\"Similarity of 'go' and 'come':\", model.similarity(\"go\", \"come\"), \"\\n\")\n",
    "\n",
    "# Dissimilar words:\n",
    "print(\"Similarity of 'go' and 'orange':\", model.similarity(\"go\", \"orange\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1883', 0.7338681221008301),\n",
       " ('Nebraska', 0.714241087436676),\n",
       " ('Buffalo', 0.7076287269592285),\n",
       " ('Horn', 0.6886376738548279),\n",
       " ('1921', 0.6870503425598145),\n",
       " ('Missouri', 0.684490442276001),\n",
       " ('Kentucky', 0.6808568835258484),\n",
       " ('Birmingham', 0.6747636795043945),\n",
       " ('Embassy', 0.6742736101150513),\n",
       " ('Plains', 0.6679900884628296)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"Korea\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10400999 -0.28723761 -0.39321992 -0.07453883 -0.16070177  0.02608893\n",
      " -0.36562774  0.53426212 -0.10667827  0.40968025 -0.08523978  0.21267067\n",
      " -0.17858098 -0.08899118 -0.3743138   0.4602769   0.00985924 -0.52807665\n",
      "  0.13695192  0.08563213 -0.37231773  0.09058453 -0.47096005  0.14891569\n",
      "  0.07235859 -0.5782941  -0.27261162  0.62884688 -0.03339595 -0.40897548\n",
      "  0.33745039 -0.09804494  0.49227691  0.37999123 -0.13616557 -0.24272589\n",
      " -0.22699361 -0.08595905 -0.34847173 -0.17083147  0.43903652 -0.1142322\n",
      " -0.04916618  0.32933396  0.08061542  0.51817828 -0.0835963  -0.23297638\n",
      " -0.23575093  0.19030149 -0.11645392 -0.1596815   0.10548559 -0.22564806\n",
      " -0.18969122 -0.16085964 -0.27025726  0.24448828 -0.07644074  0.25189272\n",
      "  0.11502693 -0.07111529  0.41744712  0.24963957 -0.12875514  0.60899067\n",
      "  0.07185046 -0.04045171  0.37370163 -0.24526568 -0.07696132  0.53539491\n",
      " -0.01748968  0.25131464 -0.05654695  0.08259479  0.14162362  0.05591664\n",
      "  0.28641215  0.15084352 -0.22377631  0.47152653 -0.15736219  0.07136656\n",
      " -0.13908115 -0.1157933   0.22627832  0.20362951  0.24959432 -0.12728594\n",
      " -0.20931536  0.00537832 -0.28624463  0.30419165 -0.2307232  -0.29763493\n",
      "  0.00921012  0.59871632 -0.11039881  0.19389735]\n"
     ]
    }
   ],
   "source": [
    "# Checking individual vectors\n",
    "orange_vector = model.wv.syn0[model.wv.vocab[\"orange\"].index]\n",
    "print(orange_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternatively, can use a pre-trained word2vec model.\n",
    "* W2V model made by Google (1.5 GB, vocabulary of 3 million words, 100 billion words total): https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "* W2V models for many different languages: https://github.com/Kyubyong/wordvectors\n",
    "* More W2V models: https://sites.google.com/site/rmyeid/projects/polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example: loading the above word2vec model by Google\n",
    "* large_model = gensim.models.KeyedVectors.load_word2vec_format('directory/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
